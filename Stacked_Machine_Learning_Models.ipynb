{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked Machine Learning Models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugw48teytIx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "aeaf2119-3c33-488a-e561-c8c6de4272dd"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "!pip install optuna\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "# sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "!pip install vecstack\n",
        "from vecstack import stacking\n",
        "\n",
        "from itertools import combinations \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "sns.set()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: cmaes>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.12.0)\n",
            "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: vecstack in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from vecstack) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->vecstack) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGGTnPorCR1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSEjKNjst_uK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98dabf2c-866c-444f-f01e-611badc22531"
      },
      "source": [
        "X, y = load_boston(return_X_y=True)\n",
        "X.shape"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1PeekNbXjIS",
        "colab_type": "text"
      },
      "source": [
        "Creating Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RFW9dvCFs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_TSNE = TSNE(n_components=2).fit_transform(X)\n",
        "# X_DBSCAN = DBSCAN(eps=3, min_samples=2).fit(X)\n",
        "# X_PCA = PCA(n_components=2).fit_transform(X)\n",
        "# add up to 1024 depending on how large your data is \n",
        "X_KNN2, indices = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "X_KNN4, indices = NearestNeighbors(n_neighbors=4, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "X_KNN8, indices = NearestNeighbors(n_neighbors=8, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN16, indices = NearestNeighbors(n_neighbors=16, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN32, indices = NearestNeighbors(n_neighbors=32, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN64, indices = NearestNeighbors(n_neighbors=64, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN128, indices = NearestNeighbors(n_neighbors=128, algorithm='ball_tree').fit(X).kneighbors(X)"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrJIhvAnDZui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b42d850b-d236-4d89-9fc9-c3a200e39605"
      },
      "source": [
        "# USING NUMPY ARRAYS\n",
        "# X = np.concatenate((X, X_TSNE), axis=1)\n",
        "# X = np.c_[ X, X_DBSCAN.labels_]\n",
        "# X = np.concatenate((X, X_PCA), axis=1)\n",
        "X = np.concatenate((X, X_KNN2[:,1::]), axis=1)\n",
        "X = np.concatenate((X, X_KNN4[:,1::]), axis=1)\n",
        "X = np.concatenate((X, X_KNN8[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN16[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN32[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN64[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN128[:,1::]), axis=1)\n",
        "X.shape\n",
        "\n",
        "# USING PANDAS \n",
        "\n",
        "\n"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbRSijDWk7TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaler_fuc(scaler):\n",
        "  train_x, valid_x, train_y, valid_y = train_test_split(X, y,\n",
        "                                              test_size=0.25, random_state = 123)\n",
        "  if (scaler == 'minmax'):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x)\n",
        "    train_x = scaler.transform(train_x)\n",
        "    scaler.fit(valid_x)\n",
        "    valid_x = scaler.transform(valid_x)\n",
        "\n",
        "  if (scaler == 'stand'):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_x)\n",
        "    train_x = scaler.transform(train_x)\n",
        "    scaler.fit(valid_x)\n",
        "    valid_x = scaler.transform(valid_x)\n",
        "\n",
        "  if (scaler == 'log'):\n",
        "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "    train_x = transformer.transform(train_x)\n",
        "    valid_x = transformer.transform(valid_x)\n",
        "\n",
        "  # turning these train/tests into lgb/xgb datasets\n",
        "  dtrain_gbm = lgb.Dataset(train_x, label=train_y)\n",
        "  dvalid_gbm = lgb.Dataset(valid_x, label=valid_y)\n",
        "\n",
        "  dtrain_xbg = xgb.DMatrix(train_x, label=train_y)\n",
        "  dvalid_xbg = xgb.DMatrix(valid_x, label=valid_y)\n",
        "  return train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvOJWxJkHWx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# models = ['lreg', 'las', 'rid','ada', 'et', 'rf', 'xgb', 'gbm'] # when you want to try all of them/ iteration: 1 \n",
        "models = ['ada', 'et', 'rf', 'xgb', 'gbm'] # selecting the 5 best models from iteration 1 (by observing the log) / iteration: 2\n",
        "comb = combinations(models, 3) \n",
        "comb = list(comb)"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozV7fL7kt9Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Objective:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.best_gbm = None\n",
        "        self._gbm = None\n",
        "        self.best_xgb = None\n",
        "        self._xgb = None\n",
        "        self.predictions = None\n",
        "        self.fpredictions = None\n",
        "\n",
        "    def __call__(self, trial):\n",
        "\n",
        "        i = trial.suggest_int(\"combos\", 0, (len(comb)-1))\n",
        " \n",
        "        gbm_preds = 0\n",
        "        gbm_rmse = float('inf')\n",
        "        xgb_preds = 0\n",
        "        xgb_rmse = float('inf')\n",
        "        rf_preds = 0\n",
        "        rf_rmse = float('inf')\n",
        "        et_preds = 0\n",
        "        et_rmse = float('inf')\n",
        "        ada_preds = 0\n",
        "        ada_rmse = float('inf')\n",
        "        lreg_preds = 0\n",
        "        lreg_rmse = float('inf')\n",
        "        las_preds = 0\n",
        "        las_rmse = float('inf')\n",
        "        rid_preds = 0\n",
        "        rid_rmse = float('inf')\n",
        "\n",
        "        ###############################################################################\n",
        "        #                                 . Scaling                                   #\n",
        "        ###############################################################################\n",
        "        # If you want 1 scaled data for everything use this and comment out, other wise leave this commented\n",
        "        # scaler = trial.suggest_categorical(\"Scaler\", ['minmax','stand','log'])\n",
        "        # train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(scaler)\n",
        "        ###############################################################################\n",
        "        #                            . Linear Regression                              #\n",
        "        ###############################################################################\n",
        "        if any(x == 'lreg' for x in comb[i]):\n",
        "          lr_scaler = trial.suggest_categorical(\"lr_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(lr_scaler)\n",
        "          lreg = LinearRegression().fit(train_x, train_y)\n",
        "          lreg_preds = lreg.predict(valid_x)\n",
        "          lreg_rmse = sklearn.metrics.mean_squared_error(valid_y, lreg_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . Lasso                                     #\n",
        "        ###############################################################################\n",
        "        if any(x == 'las' for x in comb[i]):\n",
        "          las_scaler = trial.suggest_categorical(\"las_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(las_scaler)\n",
        "          alphal = trial.suggest_loguniform(\"alphal\", 1e-8, 1.0)\n",
        "          las = linear_model.Lasso(alpha=alphal).fit(train_x, train_y)\n",
        "          las_preds = las.predict(valid_x)\n",
        "          las_rmse = sklearn.metrics.mean_squared_error(valid_y, las_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . Ridge                                     #\n",
        "        ###############################################################################\n",
        "        if any(x == 'rid' for x in comb[i]):\n",
        "          rid_scaler = trial.suggest_categorical(\"rid_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(rid_scaler)\n",
        "          alphar = trial.suggest_loguniform(\"alphar\", 1e-8, 1.0)\n",
        "          rid = Ridge(alpha=alphar).fit(train_x, train_y)\n",
        "          rid_preds = rid.predict(valid_x)\n",
        "          rid_rmse = sklearn.metrics.mean_squared_error(valid_y, rid_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                               . Ada Boost                                   #\n",
        "        ###############################################################################\n",
        "        if any(x == 'ada' for x in comb[i]):\n",
        "          ada_scaler = trial.suggest_categorical(\"ada_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(ada_scaler)\n",
        "          ada_ne = trial.suggest_int(\"ada_ne\", 1, 1000)\n",
        "          ada_lr = trial.suggest_loguniform(\"ada_lr\", 0.05, 1)\n",
        "          ada = AdaBoostRegressor(n_estimators = ada_ne, learning_rate = ada_lr,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          ada_preds = ada.predict(valid_x)\n",
        "          ada_rmse = sklearn.metrics.mean_squared_error(valid_y, ada_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                              . Extra Trees                                  #\n",
        "        ###############################################################################\n",
        "        if any(x == 'et' for x in comb[i]):\n",
        "          et_scaler = trial.suggest_categorical(\"et_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(et_scaler)\n",
        "          et_md = trial.suggest_int(\"et_max_depth\", 1, 20)\n",
        "          et_ne = trial.suggest_int(\"et_ne\", 1, 1000)\n",
        "          et = ExtraTreesRegressor(max_depth=et_md, n_estimators = et_ne,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          et_preds = et.predict(valid_x)\n",
        "          et_rmse = sklearn.metrics.mean_squared_error(valid_y, et_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                             . Random Forest                                 #\n",
        "        ###############################################################################\n",
        "        if any(x == 'rf' for x in comb[i]):\n",
        "          rf_scaler = trial.suggest_categorical(\"rf_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(rf_scaler)\n",
        "          rf_md = trial.suggest_int(\"rf_max_depth\", 1, 20)\n",
        "          rf_ne = trial.suggest_int(\"rf_ne\", 1, 1000)\n",
        "          rf = RandomForestRegressor(max_depth=rf_md, n_estimators = rf_ne,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          rf_preds = rf.predict(valid_x)\n",
        "          rf_rmse = sklearn.metrics.mean_squared_error(valid_y, rf_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . XGBoost                                   #\n",
        "        ###############################################################################\n",
        "        if any(x == 'xgb' for x in comb[i]): \n",
        "          xgb_scaler = trial.suggest_categorical(\"xgb_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(xgb_scaler)\n",
        "          xgb_param = {\n",
        "              \"silent\": 1,\n",
        "              \"objective\": \"reg:squarederror\",\n",
        "              \"eval_metric\": \"rmse\",\n",
        "              \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "              \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
        "              \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n",
        "          }\n",
        "          if xgb_param[\"booster\"] == \"gbtree\" or xgb_param[\"booster\"] == \"dart\":\n",
        "              xgb_param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
        "              xgb_param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n",
        "              xgb_param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n",
        "              xgb_param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "          if xgb_param[\"booster\"] == \"dart\":\n",
        "              xgb_param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "              xgb_param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "              xgb_param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0)\n",
        "              xgb_param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n",
        "          # Add a callback for pruning.\n",
        "          xgb_pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\" )\n",
        "          xgb_ = xgb.train(xgb_param, dtrain_xbg, evals=[(dvalid_xbg, \"validation\")], verbose_eval=False, callbacks=[xgb_pruning_callback])\n",
        "          xgb_preds = xgb_.predict(dvalid_xbg)\n",
        "          xgb_rmse = sklearn.metrics.mean_squared_error(valid_y, xgb_preds, squared = False)\n",
        "          self._xgb = xgb_\n",
        "        ###############################################################################\n",
        "        #                          . Light Gradient Boosting                          #\n",
        "        ###############################################################################\n",
        "        if any(x == 'gbm' for x in comb[i]):\n",
        "          gbm_scaler = trial.suggest_categorical(\"gbm_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(gbm_scaler)\n",
        "          gbm_param = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "              \"verbosity\": -1,\n",
        "              \"boosting_type\": \"gbdt\",\n",
        "              \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10), \n",
        "              \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10),\n",
        "              \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256), \n",
        "              \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0), \n",
        "              \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
        "              \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7), \n",
        "              \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2, 100), \n",
        "          }\n",
        "          # Add a callback for pruning.\n",
        "          gbm_pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n",
        "          gbm = lgb.train(gbm_param, dtrain_gbm, valid_sets=[dvalid_gbm], verbose_eval=False, callbacks=[gbm_pruning_callback])\n",
        "          gbm_preds = gbm.predict(valid_x)\n",
        "          gbm_rmse = sklearn.metrics.mean_squared_error(valid_y, gbm_preds, squared = False)\n",
        "          self._gbm = gbm\n",
        "        ###############################################################################\n",
        "        #                            . Stacking Strategy                              #\n",
        "        ###############################################################################\n",
        "\n",
        "        # strat = trial.suggest_int(\"strat\", 0, 1)\n",
        "\n",
        "        # if (strat == 0): # taking mean\n",
        "        preds = (gbm_preds + xgb_preds + rf_preds + \\\n",
        "        las_preds + ada_preds + et_preds + \\\n",
        "        lreg_preds + rid_preds) / 3\n",
        "\n",
        "\n",
        "        # if (strat == 1): # Making an equation\n",
        "        # a = trial.suggest_uniform(\"a\", 0.1, 0.9)\n",
        "        # b = trial.suggest_uniform(\"b\", 0.1, 0.9)\n",
        "        # c = trial.suggest_uniform(\"c\", 0.1, 0.9)\n",
        "        # d = trial.suggest_uniform(\"d\", 0.1, 0.9)\n",
        "\n",
        "        # def custom_sort(t):\n",
        "        #   return t[0]\n",
        "\n",
        "        # R = [[gbm_rmse,gbm_preds] , [xgb_rmse,xgb_preds], [rf_rmse, rf_preds], \\\n",
        "        #     [las_rmse,las_preds], [ada_rmse,ada_preds],[et_rmse,et_preds],\\\n",
        "        #     [lreg_rmse,lreg_preds], [rid_rmse,rid_preds]]\n",
        "        # R.sort(key=custom_sort) # low to high\n",
        "        # m1 = R[0][1]\n",
        "        # m2 = R[1][1]\n",
        "        # m3 = R[2][1]\n",
        "        # # Using list comprehension. This may result in a divide by 0 error when rounding occurs.\n",
        "        # # A fix would be to use a more precise library\n",
        "        # preds = [ (a*((x**b) + (y**c))) + (d*z) for x,y,z in zip(m1,m2,m3) ]\n",
        "        self.predictions = preds\n",
        "        rmse = sklearn.metrics.mean_squared_error(valid_y, preds, squared = False)\n",
        "        return rmse\n",
        "\n",
        "    def callback(self, study, trial):\n",
        "        if study.best_trial == trial:\n",
        "            self.best_gbm = self._gbm\n",
        "            self.best_xgb = self._xgb\n",
        "            self.fpredictions = self.predictions"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZdyvAQvuddv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d0241f83-5d76-4575-826c-8ae2dfb330ff"
      },
      "source": [
        "import optuna\n",
        "objective = Objective()\n",
        "\n",
        "# Setting SEED \n",
        "from optuna.samplers import TPESampler\n",
        "sampler = TPESampler(seed=10)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\",\n",
        "    sampler=sampler\n",
        ")\n",
        "study.optimize(objective, n_trials=10, callbacks=[objective.callback])\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "best_gbm = objective.best_gbm\n",
        "best_xgb = objective.best_xgb"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-07-08 16:57:53,405] Finished trial#0 with value: 4.135101019310722 with parameters: {'combos': 9, 'rf_Scaler': 'stand', 'rf_max_depth': 5, 'rf_ne': 528, 'xgb_Scaler': 'minmax', 'booster': 'gblinear', 'lambda': 0.00978207662259244, 'alpha': 9.728728830009641e-05, 'gbm_Scaler': 'minmax', 'lambda_l1': 1.684789145169785e-08, 'lambda_l2': 0.024147495209786127, 'num_leaves': 75, 'feature_fraction': 0.5014665019375213, 'bagging_fraction': 0.4530038885044062, 'bagging_freq': 5, 'min_child_samples': 18}. Best is trial#0 with value: 4.135101019310722.\n",
            "[I 2020-07-08 16:57:54,754] Finished trial#1 with value: 4.1534643875134245 with parameters: {'combos': 4, 'ada_Scaler': 'log', 'ada_ne': 345, 'ada_lr': 0.5704456208852126, 'rf_Scaler': 'log', 'rf_max_depth': 2, 'rf_ne': 201, 'gbm_Scaler': 'minmax', 'lambda_l1': 4.235304245072407e-06, 'lambda_l2': 1.8195630232649103, 'num_leaves': 79, 'feature_fraction': 0.47887489144151374, 'bagging_fraction': 0.6482004246510716, 'bagging_freq': 7, 'min_child_samples': 15}. Best is trial#0 with value: 4.135101019310722.\n",
            "[I 2020-07-08 16:57:56,482] Finished trial#2 with value: 3.989877018982281 with parameters: {'combos': 6, 'et_Scaler': 'log', 'et_max_depth': 13, 'et_ne': 322, 'rf_Scaler': 'stand', 'rf_max_depth': 5, 'rf_ne': 284, 'xgb_Scaler': 'log', 'booster': 'gblinear', 'lambda': 0.0006431490422420297, 'alpha': 0.027655758533508757}. Best is trial#2 with value: 3.989877018982281.\n",
            "[I 2020-07-08 16:57:57,086] Finished trial#3 with value: 9.183760397745576 with parameters: {'combos': 8, 'et_Scaler': 'minmax', 'et_max_depth': 11, 'et_ne': 217, 'xgb_Scaler': 'stand', 'booster': 'dart', 'lambda': 2.544488269752405e-06, 'alpha': 8.163471763379958e-08, 'max_depth': 2, 'eta': 2.3723051688905587e-08, 'gamma': 0.0010239934684337197, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.289020201319028e-07, 'skip_drop': 0.013647100628412394, 'gbm_Scaler': 'minmax', 'lambda_l1': 1.4617521957829217e-05, 'lambda_l2': 0.061919754212729834, 'num_leaves': 110, 'feature_fraction': 0.9234354759034654, 'bagging_fraction': 0.9898125201114042, 'bagging_freq': 6, 'min_child_samples': 90}. Best is trial#2 with value: 3.989877018982281.\n",
            "[I 2020-07-08 16:58:01,978] Finished trial#4 with value: 8.839558202173576 with parameters: {'combos': 6, 'et_Scaler': 'log', 'et_max_depth': 11, 'et_ne': 663, 'rf_Scaler': 'minmax', 'rf_max_depth': 13, 'rf_ne': 733, 'xgb_Scaler': 'minmax', 'booster': 'dart', 'lambda': 0.7961663152623397, 'alpha': 4.471326429846904e-05, 'max_depth': 9, 'eta': 1.0256355687768682e-06, 'gamma': 0.0006011366210390782, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 9.553436381517415e-06, 'skip_drop': 1.0453316278779107e-08}. Best is trial#2 with value: 3.989877018982281.\n",
            "[I 2020-07-08 16:58:05,380] Finished trial#5 with value: 3.517420404884898 with parameters: {'combos': 0, 'ada_Scaler': 'log', 'ada_ne': 536, 'ada_lr': 0.06346715379529481, 'et_Scaler': 'minmax', 'et_max_depth': 19, 'et_ne': 625, 'rf_Scaler': 'log', 'rf_max_depth': 20, 'rf_ne': 80}. Best is trial#5 with value: 3.517420404884898.\n",
            "[I 2020-07-08 16:58:08,306] Finished trial#6 with value: 8.444907266561273 with parameters: {'combos': 3, 'ada_Scaler': 'stand', 'ada_ne': 691, 'ada_lr': 0.128439708440937, 'rf_Scaler': 'log', 'rf_max_depth': 17, 'rf_ne': 235, 'xgb_Scaler': 'stand', 'booster': 'gbtree', 'lambda': 7.26320834194953e-07, 'alpha': 0.0002832340763026987, 'max_depth': 3, 'eta': 2.1881301522419794e-05, 'gamma': 0.0006992307438483914, 'grow_policy': 'depthwise'}. Best is trial#5 with value: 3.517420404884898.\n",
            "[I 2020-07-08 16:58:11,363] Finished trial#7 with value: 4.1257705354814584 with parameters: {'combos': 0, 'ada_Scaler': 'stand', 'ada_ne': 202, 'ada_lr': 0.1167762177192328, 'et_Scaler': 'minmax', 'et_max_depth': 7, 'et_ne': 767, 'rf_Scaler': 'log', 'rf_max_depth': 1, 'rf_ne': 835}. Best is trial#5 with value: 3.517420404884898.\n",
            "[I 2020-07-08 16:58:14,106] Finished trial#8 with value: 3.691504735081214 with parameters: {'combos': 3, 'ada_Scaler': 'log', 'ada_ne': 96, 'ada_lr': 0.2720893855967198, 'rf_Scaler': 'log', 'rf_max_depth': 14, 'rf_ne': 507, 'xgb_Scaler': 'minmax', 'booster': 'gblinear', 'lambda': 5.2199068028837845e-06, 'alpha': 9.128281974431969e-05}. Best is trial#5 with value: 3.517420404884898.\n",
            "[I 2020-07-08 16:58:19,754] Finished trial#9 with value: 3.882597107740064 with parameters: {'combos': 3, 'ada_Scaler': 'log', 'ada_ne': 422, 'ada_lr': 0.13847415272090957, 'rf_Scaler': 'stand', 'rf_max_depth': 12, 'rf_ne': 985, 'xgb_Scaler': 'minmax', 'booster': 'gblinear', 'lambda': 0.06879440955407841, 'alpha': 4.989597200979512e-06}. Best is trial#5 with value: 3.517420404884898.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Params: \n",
            "    combos: 0\n",
            "    ada_Scaler: log\n",
            "    ada_ne: 536\n",
            "    ada_lr: 0.06346715379529481\n",
            "    et_Scaler: minmax\n",
            "    et_max_depth: 19\n",
            "    et_ne: 625\n",
            "    rf_Scaler: log\n",
            "    rf_max_depth: 20\n",
            "    rf_ne: 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqw72dcpYYbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52cb37ad-b3a2-41c6-a53f-a095ccb43433"
      },
      "source": [
        "comb[1]"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ada', 'et', 'xgb')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZf0muwDLZlB",
        "colab_type": "text"
      },
      "source": [
        "best result so far:\n",
        "\n",
        "3.067 : ('ada', 'et', 'xgb')\n",
        "\n",
        "```\n",
        "  Params: \n",
        "    combos: 1\n",
        "    ada_Scaler: log\n",
        "    ada_ne: 173\n",
        "    ada_lr: 0.6716180264894074\n",
        "    et_Scaler: stand\n",
        "    et_max_depth: 10\n",
        "    et_ne: 593\n",
        "    xgb_Scaler: log\n",
        "    booster: gbtree\n",
        "    lambda: 0.056298103046901415\n",
        "    alpha: 0.0005885345642231615\n",
        "    max_depth: 3\n",
        "    eta: 0.5865796691943621\n",
        "    gamma: 6.697097340043158e-08\n",
        "    grow_policy: depthwise\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hy4db904fS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "e6801b91-54a1-410e-97ec-44ba4af5b40f"
      },
      "source": [
        "objective.fpredictions "
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([27.85772216, 27.39908626, 48.17995478, 18.33345253, 30.14070563,\n",
              "       37.29053558, 26.25861886, 10.15475066, 18.14889132, 29.62243171,\n",
              "       23.97859444, 19.98696824, 15.73101726, 26.59151738, 18.75253067,\n",
              "       20.55269833, 20.15853826, 42.75472066, 18.58739498, 15.48510911,\n",
              "       16.54584927, 45.3914744 , 35.21042146, 46.35623061, 48.51340702,\n",
              "       21.79671212, 16.1662488 , 20.632855  , 20.63631538, 21.8016087 ,\n",
              "       25.60592866, 31.17305858, 13.99109611, 21.13797821, 23.39973703,\n",
              "       32.81045667, 24.95684238, 14.94596067, 16.64715835, 47.23880925,\n",
              "       28.05968523, 19.49358329, 26.5581701 , 48.90793117, 16.9481093 ,\n",
              "       23.05092158, 19.3891282 , 22.99633326, 17.07364299, 19.91790302,\n",
              "       30.94912602, 25.8317543 , 19.39997722, 10.88814199, 20.99495711,\n",
              "       16.48914731, 13.93787858, 10.18333297, 33.5668052 , 11.20237941,\n",
              "       19.91155235, 18.3737015 , 16.89461203, 19.29234017, 21.27131705,\n",
              "       24.04901804, 24.36425727, 20.89394213, 24.4201314 , 27.94929521,\n",
              "       19.74252759, 25.33403003, 17.28790167, 25.9123166 , 16.44451764,\n",
              "       16.63531878, 11.43977836, 19.16597762, 30.52726524, 15.17126309,\n",
              "       32.39506504, 10.21351276, 24.63694802, 20.80589286, 18.22299324,\n",
              "       24.23841371, 15.87973525, 20.54756935, 20.83842337, 29.64733111,\n",
              "       14.48433205, 32.5303475 , 19.02837872, 26.12504384, 33.70861763,\n",
              "       29.56191121, 15.9919719 , 30.69012391, 26.0483593 , 34.55985829,\n",
              "       20.42365766, 15.73423696, 48.09816664, 14.97174075, 20.03645869,\n",
              "       26.77140724, 18.08230588, 14.55989022, 19.70093752, 24.76928705,\n",
              "       23.05843666, 19.88289464, 14.15439795, 21.13339319, 18.87177318,\n",
              "       26.25289119, 26.30468377, 14.23937913, 20.15280709, 18.61350579,\n",
              "       20.87413916, 35.49068727, 21.88393918, 11.41379609, 15.60508544,\n",
              "       20.5535389 , 22.31060219])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    }
  ]
}