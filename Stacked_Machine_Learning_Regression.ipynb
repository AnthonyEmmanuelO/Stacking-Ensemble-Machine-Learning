{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked Machine Learning Regression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugw48teytIx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "491b02c6-69fd-44c6-eabc-eca297a9345e"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "!pip install optuna\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "# sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "!pip install vecstack\n",
        "from vecstack import stacking\n",
        "\n",
        "from itertools import combinations \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "sns.set()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
            "Requirement already satisfied: cmaes>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.12.0)\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.0.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
            "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=1.20.0->cliff->optuna) (1.7.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=1.20.0->cliff->optuna) (3.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vecstack in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from vecstack) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->vecstack) (0.15.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGGTnPorCR1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSEjKNjst_uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X, y = load_boston(return_X_y=True)\n",
        "# X.shape"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmEJ2LllEl7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "f5e91fcb-47cb-4db6-f2e4-9a0b04c35900"
      },
      "source": [
        "import os, ssl\n",
        "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
        "getattr(ssl, '_create_unverified_context', None)):\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "url = 'https://www.cryptodatadownload.com/cdd/Binance_ETHUSDT_1h.csv'\n",
        "data = pd.read_csv(url,header=1)\n",
        "data = data.loc[:,['Date','Open',\t'High',\t'Low', 'Close',\t'Volume ETH']]\n",
        "data = data.rename(columns={\"Volume ETH\": \"Volume\"})\n",
        "data.index = data['Date']\n",
        "data = data.drop('Date', axis = 1)\n",
        "data = data.iloc[::-1]\n",
        "data.tail()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-07-08 10-AM</th>\n",
              "      <td>241.09</td>\n",
              "      <td>244.27</td>\n",
              "      <td>241.09</td>\n",
              "      <td>243.60</td>\n",
              "      <td>59031.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 11-AM</th>\n",
              "      <td>243.60</td>\n",
              "      <td>244.60</td>\n",
              "      <td>242.48</td>\n",
              "      <td>244.07</td>\n",
              "      <td>51011.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 12-PM</th>\n",
              "      <td>244.07</td>\n",
              "      <td>244.70</td>\n",
              "      <td>241.28</td>\n",
              "      <td>242.06</td>\n",
              "      <td>59565.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 01-PM</th>\n",
              "      <td>242.06</td>\n",
              "      <td>246.90</td>\n",
              "      <td>242.00</td>\n",
              "      <td>246.32</td>\n",
              "      <td>91679.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 02-PM</th>\n",
              "      <td>246.32</td>\n",
              "      <td>247.63</td>\n",
              "      <td>245.64</td>\n",
              "      <td>246.14</td>\n",
              "      <td>48127.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Open    High     Low   Close    Volume\n",
              "Date                                                      \n",
              "2020-07-08 10-AM  241.09  244.27  241.09  243.60  59031.54\n",
              "2020-07-08 11-AM  243.60  244.60  242.48  244.07  51011.61\n",
              "2020-07-08 12-PM  244.07  244.70  241.28  242.06  59565.50\n",
              "2020-07-08 01-PM  242.06  246.90  242.00  246.32  91679.67\n",
              "2020-07-08 02-PM  246.32  247.63  245.64  246.14  48127.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVsD_dO9Eo9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "531787c9-8696-4071-ca3d-627af96cc254"
      },
      "source": [
        "def get_indicators(data):\n",
        "    # Previous Candle\n",
        "    for i in np.arange(1,21,1):\n",
        "      data[f'Close_lag{i}'] = data['Close'].shift(i)\n",
        "      data[f'Volume_lag{i}'] = data['Volume'].shift(i)\n",
        "\n",
        "\n",
        "    data['target'] = data['Close'] #.shift(-1)\n",
        "    data = data.drop(['Open',\t'High',\t'Low', 'Close',\t'Volume'], axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "data = get_indicators(data)\n",
        "data = data[data['Close_lag20'].notna()]\n",
        "data.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close_lag1</th>\n",
              "      <th>Volume_lag1</th>\n",
              "      <th>Close_lag2</th>\n",
              "      <th>Volume_lag2</th>\n",
              "      <th>Close_lag3</th>\n",
              "      <th>Volume_lag3</th>\n",
              "      <th>Close_lag4</th>\n",
              "      <th>Volume_lag4</th>\n",
              "      <th>Close_lag5</th>\n",
              "      <th>Volume_lag5</th>\n",
              "      <th>Close_lag6</th>\n",
              "      <th>Volume_lag6</th>\n",
              "      <th>Close_lag7</th>\n",
              "      <th>Volume_lag7</th>\n",
              "      <th>Close_lag8</th>\n",
              "      <th>Volume_lag8</th>\n",
              "      <th>Close_lag9</th>\n",
              "      <th>Volume_lag9</th>\n",
              "      <th>Close_lag10</th>\n",
              "      <th>Volume_lag10</th>\n",
              "      <th>Close_lag11</th>\n",
              "      <th>Volume_lag11</th>\n",
              "      <th>Close_lag12</th>\n",
              "      <th>Volume_lag12</th>\n",
              "      <th>Close_lag13</th>\n",
              "      <th>Volume_lag13</th>\n",
              "      <th>Close_lag14</th>\n",
              "      <th>Volume_lag14</th>\n",
              "      <th>Close_lag15</th>\n",
              "      <th>Volume_lag15</th>\n",
              "      <th>Close_lag16</th>\n",
              "      <th>Volume_lag16</th>\n",
              "      <th>Close_lag17</th>\n",
              "      <th>Volume_lag17</th>\n",
              "      <th>Close_lag18</th>\n",
              "      <th>Volume_lag18</th>\n",
              "      <th>Close_lag19</th>\n",
              "      <th>Volume_lag19</th>\n",
              "      <th>Close_lag20</th>\n",
              "      <th>Volume_lag20</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-08-18 12-AM</th>\n",
              "      <td>303.02</td>\n",
              "      <td>119.21</td>\n",
              "      <td>306.74</td>\n",
              "      <td>368.37</td>\n",
              "      <td>307.44</td>\n",
              "      <td>330.55</td>\n",
              "      <td>304.79</td>\n",
              "      <td>270.32</td>\n",
              "      <td>309.41</td>\n",
              "      <td>215.64</td>\n",
              "      <td>309.10</td>\n",
              "      <td>203.65</td>\n",
              "      <td>308.33</td>\n",
              "      <td>191.95</td>\n",
              "      <td>308.00</td>\n",
              "      <td>596.80</td>\n",
              "      <td>307.06</td>\n",
              "      <td>459.42</td>\n",
              "      <td>306.50</td>\n",
              "      <td>695.46</td>\n",
              "      <td>309.40</td>\n",
              "      <td>310.58</td>\n",
              "      <td>308.62</td>\n",
              "      <td>401.78</td>\n",
              "      <td>308.95</td>\n",
              "      <td>271.06</td>\n",
              "      <td>309.30</td>\n",
              "      <td>284.39</td>\n",
              "      <td>310.00</td>\n",
              "      <td>464.18</td>\n",
              "      <td>308.62</td>\n",
              "      <td>150.75</td>\n",
              "      <td>307.96</td>\n",
              "      <td>753.19</td>\n",
              "      <td>302.68</td>\n",
              "      <td>302.51</td>\n",
              "      <td>303.10</td>\n",
              "      <td>377.67</td>\n",
              "      <td>300.79</td>\n",
              "      <td>122.52</td>\n",
              "      <td>301.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-18 01-AM</th>\n",
              "      <td>301.19</td>\n",
              "      <td>496.43</td>\n",
              "      <td>303.02</td>\n",
              "      <td>119.21</td>\n",
              "      <td>306.74</td>\n",
              "      <td>368.37</td>\n",
              "      <td>307.44</td>\n",
              "      <td>330.55</td>\n",
              "      <td>304.79</td>\n",
              "      <td>270.32</td>\n",
              "      <td>309.41</td>\n",
              "      <td>215.64</td>\n",
              "      <td>309.10</td>\n",
              "      <td>203.65</td>\n",
              "      <td>308.33</td>\n",
              "      <td>191.95</td>\n",
              "      <td>308.00</td>\n",
              "      <td>596.80</td>\n",
              "      <td>307.06</td>\n",
              "      <td>459.42</td>\n",
              "      <td>306.50</td>\n",
              "      <td>695.46</td>\n",
              "      <td>309.40</td>\n",
              "      <td>310.58</td>\n",
              "      <td>308.62</td>\n",
              "      <td>401.78</td>\n",
              "      <td>308.95</td>\n",
              "      <td>271.06</td>\n",
              "      <td>309.30</td>\n",
              "      <td>284.39</td>\n",
              "      <td>310.00</td>\n",
              "      <td>464.18</td>\n",
              "      <td>308.62</td>\n",
              "      <td>150.75</td>\n",
              "      <td>307.96</td>\n",
              "      <td>753.19</td>\n",
              "      <td>302.68</td>\n",
              "      <td>302.51</td>\n",
              "      <td>303.10</td>\n",
              "      <td>377.67</td>\n",
              "      <td>298.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-18 02-AM</th>\n",
              "      <td>298.73</td>\n",
              "      <td>320.05</td>\n",
              "      <td>301.19</td>\n",
              "      <td>496.43</td>\n",
              "      <td>303.02</td>\n",
              "      <td>119.21</td>\n",
              "      <td>306.74</td>\n",
              "      <td>368.37</td>\n",
              "      <td>307.44</td>\n",
              "      <td>330.55</td>\n",
              "      <td>304.79</td>\n",
              "      <td>270.32</td>\n",
              "      <td>309.41</td>\n",
              "      <td>215.64</td>\n",
              "      <td>309.10</td>\n",
              "      <td>203.65</td>\n",
              "      <td>308.33</td>\n",
              "      <td>191.95</td>\n",
              "      <td>308.00</td>\n",
              "      <td>596.80</td>\n",
              "      <td>307.06</td>\n",
              "      <td>459.42</td>\n",
              "      <td>306.50</td>\n",
              "      <td>695.46</td>\n",
              "      <td>309.40</td>\n",
              "      <td>310.58</td>\n",
              "      <td>308.62</td>\n",
              "      <td>401.78</td>\n",
              "      <td>308.95</td>\n",
              "      <td>271.06</td>\n",
              "      <td>309.30</td>\n",
              "      <td>284.39</td>\n",
              "      <td>310.00</td>\n",
              "      <td>464.18</td>\n",
              "      <td>308.62</td>\n",
              "      <td>150.75</td>\n",
              "      <td>307.96</td>\n",
              "      <td>753.19</td>\n",
              "      <td>302.68</td>\n",
              "      <td>302.51</td>\n",
              "      <td>300.52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Close_lag1  Volume_lag1  ...  Volume_lag20  target\n",
              "Date                                       ...                      \n",
              "2017-08-18 12-AM      303.02       119.21  ...        122.52  301.19\n",
              "2017-08-18 01-AM      301.19       496.43  ...        377.67  298.73\n",
              "2017-08-18 02-AM      298.73       320.05  ...        302.51  300.52\n",
              "\n",
              "[3 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOuUABzjLByO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1:].values.ravel()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDVmx-AkLY0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "546d3aa5-f348-48a5-ca10-323691a6ad67"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25335, 40)\n",
            "(25335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1PeekNbXjIS",
        "colab_type": "text"
      },
      "source": [
        "Creating Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RFW9dvCFs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_TSNE = TSNE(n_components=2).fit_transform(X)\n",
        "X_DBSCAN = DBSCAN(eps=3, min_samples=2).fit(X)\n",
        "X_PCA = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "\n",
        "# add up to 1024 depending on how large your data is PICK 1 \n",
        "# X_KNN2, indices = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN4, indices = NearestNeighbors(n_neighbors=4, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN8, indices = NearestNeighbors(n_neighbors=8, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN16, indices = NearestNeighbors(n_neighbors=16, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN32, indices = NearestNeighbors(n_neighbors=32, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "X_KNN64, indices = NearestNeighbors(n_neighbors=64, algorithm='ball_tree').fit(X).kneighbors(X)\n",
        "# X_KNN128, indices = NearestNeighbors(n_neighbors=128, algorithm='ball_tree').fit(X).kneighbors(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPaXcGxrM6cF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e30fec88-4f4f-486d-fdcd-005bc4e486ab"
      },
      "source": [
        "# USING PANDAS\n",
        "\n",
        "# KNN : YOU ONLY NEED TO PICK 1 OF THE KNNs ABOVE.\n",
        "\n",
        "# X['KNN2'] = X_KNN2[:,1::]\n",
        "\n",
        "# for i in np.arange(1,4,1):\n",
        "#   X[f'KNN4_{i}'] = X_KNN4[:,i]\n",
        "\n",
        "for i in np.arange(1,64,1):\n",
        "  X[f'KNN64_{i}'] = X_KNN64[:,i]\n",
        "\n",
        "# replace na values\n",
        "\n",
        "# for i in np.arange(0,2,1):\n",
        "#   X[f'TSNE_{i}'] = X_TSNE[:,i]\n",
        "\n",
        "# X['DBSCAN'] = X_DBSCAN.labels_\n",
        "\n",
        "# for i in np.arange(0,2,1):\n",
        "#   X[f'PCA_{i}'] = X_PCA[:,i]\n",
        "\n",
        "\n",
        "X.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25335, 103)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkCcTaX1OIBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "1d86d6b8-79c8-4c83-9aa6-6a6cec449844"
      },
      "source": [
        "X.tail()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close_lag1</th>\n",
              "      <th>Volume_lag1</th>\n",
              "      <th>Close_lag2</th>\n",
              "      <th>Volume_lag2</th>\n",
              "      <th>Close_lag3</th>\n",
              "      <th>Volume_lag3</th>\n",
              "      <th>Close_lag4</th>\n",
              "      <th>Volume_lag4</th>\n",
              "      <th>Close_lag5</th>\n",
              "      <th>Volume_lag5</th>\n",
              "      <th>Close_lag6</th>\n",
              "      <th>Volume_lag6</th>\n",
              "      <th>Close_lag7</th>\n",
              "      <th>Volume_lag7</th>\n",
              "      <th>Close_lag8</th>\n",
              "      <th>Volume_lag8</th>\n",
              "      <th>Close_lag9</th>\n",
              "      <th>Volume_lag9</th>\n",
              "      <th>Close_lag10</th>\n",
              "      <th>Volume_lag10</th>\n",
              "      <th>Close_lag11</th>\n",
              "      <th>Volume_lag11</th>\n",
              "      <th>Close_lag12</th>\n",
              "      <th>Volume_lag12</th>\n",
              "      <th>Close_lag13</th>\n",
              "      <th>Volume_lag13</th>\n",
              "      <th>Close_lag14</th>\n",
              "      <th>Volume_lag14</th>\n",
              "      <th>Close_lag15</th>\n",
              "      <th>Volume_lag15</th>\n",
              "      <th>Close_lag16</th>\n",
              "      <th>Volume_lag16</th>\n",
              "      <th>Close_lag17</th>\n",
              "      <th>Volume_lag17</th>\n",
              "      <th>Close_lag18</th>\n",
              "      <th>Volume_lag18</th>\n",
              "      <th>Close_lag19</th>\n",
              "      <th>Volume_lag19</th>\n",
              "      <th>Close_lag20</th>\n",
              "      <th>Volume_lag20</th>\n",
              "      <th>...</th>\n",
              "      <th>KNN64_29</th>\n",
              "      <th>KNN64_30</th>\n",
              "      <th>KNN64_31</th>\n",
              "      <th>KNN64_32</th>\n",
              "      <th>KNN64_33</th>\n",
              "      <th>KNN64_34</th>\n",
              "      <th>KNN64_35</th>\n",
              "      <th>KNN64_36</th>\n",
              "      <th>KNN64_37</th>\n",
              "      <th>KNN64_38</th>\n",
              "      <th>KNN64_39</th>\n",
              "      <th>KNN64_40</th>\n",
              "      <th>KNN64_41</th>\n",
              "      <th>KNN64_42</th>\n",
              "      <th>KNN64_43</th>\n",
              "      <th>KNN64_44</th>\n",
              "      <th>KNN64_45</th>\n",
              "      <th>KNN64_46</th>\n",
              "      <th>KNN64_47</th>\n",
              "      <th>KNN64_48</th>\n",
              "      <th>KNN64_49</th>\n",
              "      <th>KNN64_50</th>\n",
              "      <th>KNN64_51</th>\n",
              "      <th>KNN64_52</th>\n",
              "      <th>KNN64_53</th>\n",
              "      <th>KNN64_54</th>\n",
              "      <th>KNN64_55</th>\n",
              "      <th>KNN64_56</th>\n",
              "      <th>KNN64_57</th>\n",
              "      <th>KNN64_58</th>\n",
              "      <th>KNN64_59</th>\n",
              "      <th>KNN64_60</th>\n",
              "      <th>KNN64_61</th>\n",
              "      <th>KNN64_62</th>\n",
              "      <th>KNN64_63</th>\n",
              "      <th>TSNE_0</th>\n",
              "      <th>TSNE_1</th>\n",
              "      <th>DBSCAN</th>\n",
              "      <th>PCA_0</th>\n",
              "      <th>PCA_1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-07-08 10-AM</th>\n",
              "      <td>241.09</td>\n",
              "      <td>26613.96</td>\n",
              "      <td>240.76</td>\n",
              "      <td>25039.21</td>\n",
              "      <td>240.76</td>\n",
              "      <td>35187.07</td>\n",
              "      <td>239.99</td>\n",
              "      <td>16661.54</td>\n",
              "      <td>239.19</td>\n",
              "      <td>25651.65</td>\n",
              "      <td>239.20</td>\n",
              "      <td>58665.56</td>\n",
              "      <td>241.90</td>\n",
              "      <td>36051.36</td>\n",
              "      <td>241.47</td>\n",
              "      <td>50633.30</td>\n",
              "      <td>240.12</td>\n",
              "      <td>37584.31</td>\n",
              "      <td>238.46</td>\n",
              "      <td>22333.50</td>\n",
              "      <td>239.39</td>\n",
              "      <td>18448.02</td>\n",
              "      <td>238.93</td>\n",
              "      <td>15342.06</td>\n",
              "      <td>238.13</td>\n",
              "      <td>13973.63</td>\n",
              "      <td>238.49</td>\n",
              "      <td>24130.58</td>\n",
              "      <td>237.98</td>\n",
              "      <td>11389.74</td>\n",
              "      <td>238.90</td>\n",
              "      <td>13262.34</td>\n",
              "      <td>239.87</td>\n",
              "      <td>19892.30</td>\n",
              "      <td>238.42</td>\n",
              "      <td>27691.97</td>\n",
              "      <td>238.82</td>\n",
              "      <td>25898.23</td>\n",
              "      <td>237.20</td>\n",
              "      <td>56615.60</td>\n",
              "      <td>...</td>\n",
              "      <td>55390.743042</td>\n",
              "      <td>55421.722085</td>\n",
              "      <td>55479.903551</td>\n",
              "      <td>55513.853873</td>\n",
              "      <td>55734.536165</td>\n",
              "      <td>55872.599281</td>\n",
              "      <td>55983.645458</td>\n",
              "      <td>56240.174934</td>\n",
              "      <td>56311.348089</td>\n",
              "      <td>56485.300098</td>\n",
              "      <td>56604.847288</td>\n",
              "      <td>56654.583764</td>\n",
              "      <td>56656.334276</td>\n",
              "      <td>56763.344919</td>\n",
              "      <td>56805.438802</td>\n",
              "      <td>56958.478325</td>\n",
              "      <td>57139.913168</td>\n",
              "      <td>57197.046647</td>\n",
              "      <td>57312.512940</td>\n",
              "      <td>57422.846385</td>\n",
              "      <td>57565.204659</td>\n",
              "      <td>57638.467261</td>\n",
              "      <td>57945.455498</td>\n",
              "      <td>57960.927118</td>\n",
              "      <td>58337.766226</td>\n",
              "      <td>58384.331120</td>\n",
              "      <td>58522.915196</td>\n",
              "      <td>58618.257593</td>\n",
              "      <td>58656.918935</td>\n",
              "      <td>58663.302668</td>\n",
              "      <td>58742.013754</td>\n",
              "      <td>58860.871034</td>\n",
              "      <td>58906.373102</td>\n",
              "      <td>59035.430038</td>\n",
              "      <td>59049.821905</td>\n",
              "      <td>13.549782</td>\n",
              "      <td>40.279243</td>\n",
              "      <td>-1</td>\n",
              "      <td>60110.995059</td>\n",
              "      <td>21411.725651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 11-AM</th>\n",
              "      <td>243.60</td>\n",
              "      <td>59031.54</td>\n",
              "      <td>241.09</td>\n",
              "      <td>26613.96</td>\n",
              "      <td>240.76</td>\n",
              "      <td>25039.21</td>\n",
              "      <td>240.76</td>\n",
              "      <td>35187.07</td>\n",
              "      <td>239.99</td>\n",
              "      <td>16661.54</td>\n",
              "      <td>239.19</td>\n",
              "      <td>25651.65</td>\n",
              "      <td>239.20</td>\n",
              "      <td>58665.56</td>\n",
              "      <td>241.90</td>\n",
              "      <td>36051.36</td>\n",
              "      <td>241.47</td>\n",
              "      <td>50633.30</td>\n",
              "      <td>240.12</td>\n",
              "      <td>37584.31</td>\n",
              "      <td>238.46</td>\n",
              "      <td>22333.50</td>\n",
              "      <td>239.39</td>\n",
              "      <td>18448.02</td>\n",
              "      <td>238.93</td>\n",
              "      <td>15342.06</td>\n",
              "      <td>238.13</td>\n",
              "      <td>13973.63</td>\n",
              "      <td>238.49</td>\n",
              "      <td>24130.58</td>\n",
              "      <td>237.98</td>\n",
              "      <td>11389.74</td>\n",
              "      <td>238.90</td>\n",
              "      <td>13262.34</td>\n",
              "      <td>239.87</td>\n",
              "      <td>19892.30</td>\n",
              "      <td>238.42</td>\n",
              "      <td>27691.97</td>\n",
              "      <td>238.82</td>\n",
              "      <td>25898.23</td>\n",
              "      <td>...</td>\n",
              "      <td>58009.078118</td>\n",
              "      <td>58037.373946</td>\n",
              "      <td>58119.123856</td>\n",
              "      <td>58123.374270</td>\n",
              "      <td>58138.090448</td>\n",
              "      <td>58189.772745</td>\n",
              "      <td>58306.980872</td>\n",
              "      <td>58549.852081</td>\n",
              "      <td>58599.249136</td>\n",
              "      <td>58610.632610</td>\n",
              "      <td>58622.505921</td>\n",
              "      <td>58729.322460</td>\n",
              "      <td>58795.191343</td>\n",
              "      <td>58856.073826</td>\n",
              "      <td>58924.361940</td>\n",
              "      <td>58940.573601</td>\n",
              "      <td>59032.516957</td>\n",
              "      <td>59075.453740</td>\n",
              "      <td>59289.677772</td>\n",
              "      <td>59335.554637</td>\n",
              "      <td>59352.656477</td>\n",
              "      <td>59419.923832</td>\n",
              "      <td>59446.767621</td>\n",
              "      <td>59484.873354</td>\n",
              "      <td>59554.921668</td>\n",
              "      <td>59577.566802</td>\n",
              "      <td>59604.203088</td>\n",
              "      <td>59628.875442</td>\n",
              "      <td>59635.057100</td>\n",
              "      <td>59789.050766</td>\n",
              "      <td>59913.002132</td>\n",
              "      <td>60152.035526</td>\n",
              "      <td>60165.066061</td>\n",
              "      <td>60351.980219</td>\n",
              "      <td>60385.844103</td>\n",
              "      <td>10.269239</td>\n",
              "      <td>40.784504</td>\n",
              "      <td>-1</td>\n",
              "      <td>60711.434913</td>\n",
              "      <td>33099.896533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 12-PM</th>\n",
              "      <td>244.07</td>\n",
              "      <td>51011.61</td>\n",
              "      <td>243.60</td>\n",
              "      <td>59031.54</td>\n",
              "      <td>241.09</td>\n",
              "      <td>26613.96</td>\n",
              "      <td>240.76</td>\n",
              "      <td>25039.21</td>\n",
              "      <td>240.76</td>\n",
              "      <td>35187.07</td>\n",
              "      <td>239.99</td>\n",
              "      <td>16661.54</td>\n",
              "      <td>239.19</td>\n",
              "      <td>25651.65</td>\n",
              "      <td>239.20</td>\n",
              "      <td>58665.56</td>\n",
              "      <td>241.90</td>\n",
              "      <td>36051.36</td>\n",
              "      <td>241.47</td>\n",
              "      <td>50633.30</td>\n",
              "      <td>240.12</td>\n",
              "      <td>37584.31</td>\n",
              "      <td>238.46</td>\n",
              "      <td>22333.50</td>\n",
              "      <td>239.39</td>\n",
              "      <td>18448.02</td>\n",
              "      <td>238.93</td>\n",
              "      <td>15342.06</td>\n",
              "      <td>238.13</td>\n",
              "      <td>13973.63</td>\n",
              "      <td>238.49</td>\n",
              "      <td>24130.58</td>\n",
              "      <td>237.98</td>\n",
              "      <td>11389.74</td>\n",
              "      <td>238.90</td>\n",
              "      <td>13262.34</td>\n",
              "      <td>239.87</td>\n",
              "      <td>19892.30</td>\n",
              "      <td>238.42</td>\n",
              "      <td>27691.97</td>\n",
              "      <td>...</td>\n",
              "      <td>60549.563996</td>\n",
              "      <td>60590.267376</td>\n",
              "      <td>60751.791503</td>\n",
              "      <td>60771.024477</td>\n",
              "      <td>60805.016405</td>\n",
              "      <td>60816.652279</td>\n",
              "      <td>60944.533720</td>\n",
              "      <td>61090.866645</td>\n",
              "      <td>61537.469177</td>\n",
              "      <td>61645.380133</td>\n",
              "      <td>61678.974779</td>\n",
              "      <td>61778.746759</td>\n",
              "      <td>61817.816792</td>\n",
              "      <td>61917.143766</td>\n",
              "      <td>62103.648386</td>\n",
              "      <td>62113.996735</td>\n",
              "      <td>62156.723466</td>\n",
              "      <td>62262.900297</td>\n",
              "      <td>62491.445136</td>\n",
              "      <td>62575.353624</td>\n",
              "      <td>62634.837868</td>\n",
              "      <td>62686.931148</td>\n",
              "      <td>62778.539993</td>\n",
              "      <td>62832.844501</td>\n",
              "      <td>63155.086679</td>\n",
              "      <td>63176.888062</td>\n",
              "      <td>63233.963982</td>\n",
              "      <td>63265.198782</td>\n",
              "      <td>63301.307206</td>\n",
              "      <td>63304.742688</td>\n",
              "      <td>63329.095844</td>\n",
              "      <td>63338.869086</td>\n",
              "      <td>63356.429343</td>\n",
              "      <td>63497.509547</td>\n",
              "      <td>63557.418434</td>\n",
              "      <td>10.729051</td>\n",
              "      <td>41.372658</td>\n",
              "      <td>-1</td>\n",
              "      <td>66334.221015</td>\n",
              "      <td>36936.069516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 01-PM</th>\n",
              "      <td>242.06</td>\n",
              "      <td>59565.50</td>\n",
              "      <td>244.07</td>\n",
              "      <td>51011.61</td>\n",
              "      <td>243.60</td>\n",
              "      <td>59031.54</td>\n",
              "      <td>241.09</td>\n",
              "      <td>26613.96</td>\n",
              "      <td>240.76</td>\n",
              "      <td>25039.21</td>\n",
              "      <td>240.76</td>\n",
              "      <td>35187.07</td>\n",
              "      <td>239.99</td>\n",
              "      <td>16661.54</td>\n",
              "      <td>239.19</td>\n",
              "      <td>25651.65</td>\n",
              "      <td>239.20</td>\n",
              "      <td>58665.56</td>\n",
              "      <td>241.90</td>\n",
              "      <td>36051.36</td>\n",
              "      <td>241.47</td>\n",
              "      <td>50633.30</td>\n",
              "      <td>240.12</td>\n",
              "      <td>37584.31</td>\n",
              "      <td>238.46</td>\n",
              "      <td>22333.50</td>\n",
              "      <td>239.39</td>\n",
              "      <td>18448.02</td>\n",
              "      <td>238.93</td>\n",
              "      <td>15342.06</td>\n",
              "      <td>238.13</td>\n",
              "      <td>13973.63</td>\n",
              "      <td>238.49</td>\n",
              "      <td>24130.58</td>\n",
              "      <td>237.98</td>\n",
              "      <td>11389.74</td>\n",
              "      <td>238.90</td>\n",
              "      <td>13262.34</td>\n",
              "      <td>239.87</td>\n",
              "      <td>19892.30</td>\n",
              "      <td>...</td>\n",
              "      <td>65181.808810</td>\n",
              "      <td>65293.007473</td>\n",
              "      <td>65376.760393</td>\n",
              "      <td>65383.924785</td>\n",
              "      <td>65841.383049</td>\n",
              "      <td>66137.542100</td>\n",
              "      <td>66763.917034</td>\n",
              "      <td>66881.250158</td>\n",
              "      <td>66967.780509</td>\n",
              "      <td>66977.901464</td>\n",
              "      <td>67059.200757</td>\n",
              "      <td>67129.093101</td>\n",
              "      <td>67325.970436</td>\n",
              "      <td>67336.473401</td>\n",
              "      <td>67365.726590</td>\n",
              "      <td>67453.948974</td>\n",
              "      <td>67479.765158</td>\n",
              "      <td>67744.629030</td>\n",
              "      <td>67924.743516</td>\n",
              "      <td>68090.975053</td>\n",
              "      <td>68097.317218</td>\n",
              "      <td>68209.234759</td>\n",
              "      <td>68498.569233</td>\n",
              "      <td>68665.087117</td>\n",
              "      <td>68721.621159</td>\n",
              "      <td>68840.191042</td>\n",
              "      <td>68865.313905</td>\n",
              "      <td>68989.717722</td>\n",
              "      <td>69141.277123</td>\n",
              "      <td>69330.202794</td>\n",
              "      <td>69333.180417</td>\n",
              "      <td>69334.569733</td>\n",
              "      <td>69402.178644</td>\n",
              "      <td>69514.001014</td>\n",
              "      <td>69534.112021</td>\n",
              "      <td>39.947582</td>\n",
              "      <td>15.761859</td>\n",
              "      <td>-1</td>\n",
              "      <td>73505.405064</td>\n",
              "      <td>41808.187798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-08 02-PM</th>\n",
              "      <td>246.32</td>\n",
              "      <td>91679.67</td>\n",
              "      <td>242.06</td>\n",
              "      <td>59565.50</td>\n",
              "      <td>244.07</td>\n",
              "      <td>51011.61</td>\n",
              "      <td>243.60</td>\n",
              "      <td>59031.54</td>\n",
              "      <td>241.09</td>\n",
              "      <td>26613.96</td>\n",
              "      <td>240.76</td>\n",
              "      <td>25039.21</td>\n",
              "      <td>240.76</td>\n",
              "      <td>35187.07</td>\n",
              "      <td>239.99</td>\n",
              "      <td>16661.54</td>\n",
              "      <td>239.19</td>\n",
              "      <td>25651.65</td>\n",
              "      <td>239.20</td>\n",
              "      <td>58665.56</td>\n",
              "      <td>241.90</td>\n",
              "      <td>36051.36</td>\n",
              "      <td>241.47</td>\n",
              "      <td>50633.30</td>\n",
              "      <td>240.12</td>\n",
              "      <td>37584.31</td>\n",
              "      <td>238.46</td>\n",
              "      <td>22333.50</td>\n",
              "      <td>239.39</td>\n",
              "      <td>18448.02</td>\n",
              "      <td>238.93</td>\n",
              "      <td>15342.06</td>\n",
              "      <td>238.13</td>\n",
              "      <td>13973.63</td>\n",
              "      <td>238.49</td>\n",
              "      <td>24130.58</td>\n",
              "      <td>237.98</td>\n",
              "      <td>11389.74</td>\n",
              "      <td>238.90</td>\n",
              "      <td>13262.34</td>\n",
              "      <td>...</td>\n",
              "      <td>78615.380989</td>\n",
              "      <td>78838.684486</td>\n",
              "      <td>78848.138637</td>\n",
              "      <td>79131.370705</td>\n",
              "      <td>79694.945584</td>\n",
              "      <td>80232.809532</td>\n",
              "      <td>80276.548733</td>\n",
              "      <td>80605.680584</td>\n",
              "      <td>81144.789966</td>\n",
              "      <td>81286.362581</td>\n",
              "      <td>81460.971393</td>\n",
              "      <td>81631.311981</td>\n",
              "      <td>81705.163970</td>\n",
              "      <td>81871.235016</td>\n",
              "      <td>82034.433496</td>\n",
              "      <td>82124.890767</td>\n",
              "      <td>82295.594787</td>\n",
              "      <td>82818.352072</td>\n",
              "      <td>82858.581473</td>\n",
              "      <td>82877.249776</td>\n",
              "      <td>82902.583418</td>\n",
              "      <td>82916.556041</td>\n",
              "      <td>83110.258020</td>\n",
              "      <td>83132.283573</td>\n",
              "      <td>83189.945782</td>\n",
              "      <td>83347.284348</td>\n",
              "      <td>83393.673491</td>\n",
              "      <td>83457.978833</td>\n",
              "      <td>83613.317212</td>\n",
              "      <td>83785.885490</td>\n",
              "      <td>83794.127292</td>\n",
              "      <td>83797.190548</td>\n",
              "      <td>83825.165157</td>\n",
              "      <td>84480.353296</td>\n",
              "      <td>84518.093849</td>\n",
              "      <td>42.025635</td>\n",
              "      <td>7.849325</td>\n",
              "      <td>-1</td>\n",
              "      <td>89277.778617</td>\n",
              "      <td>51322.432368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 108 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Close_lag1  Volume_lag1  ...         PCA_0         PCA_1\n",
              "Date                                       ...                            \n",
              "2020-07-08 10-AM      241.09     26613.96  ...  60110.995059  21411.725651\n",
              "2020-07-08 11-AM      243.60     59031.54  ...  60711.434913  33099.896533\n",
              "2020-07-08 12-PM      244.07     51011.61  ...  66334.221015  36936.069516\n",
              "2020-07-08 01-PM      242.06     59565.50  ...  73505.405064  41808.187798\n",
              "2020-07-08 02-PM      246.32     91679.67  ...  89277.778617  51322.432368\n",
              "\n",
              "[5 rows x 108 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrJIhvAnDZui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USING NUMPY ARRAYS\n",
        "# X = np.concatenate((X, X_TSNE), axis=1)\n",
        "# X = np.c_[ X, X_DBSCAN.labels_]\n",
        "# X = np.concatenate((X, X_PCA), axis=1)\n",
        "# X = np.concatenate((X, X_KNN2[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN4[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN8[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN16[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN32[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN64[:,1::]), axis=1)\n",
        "# X = np.concatenate((X, X_KNN128[:,1::]), axis=1)\n",
        "# X.shape\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbRSijDWk7TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaler_fuc(scaler):\n",
        "  train_x, valid_x, train_y, valid_y = train_test_split(X, y,\n",
        "                                              test_size=0.25, random_state = 123)\n",
        "  if (scaler == 'minmax'):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x)\n",
        "    train_x = scaler.transform(train_x)\n",
        "    scaler.fit(valid_x)\n",
        "    valid_x = scaler.transform(valid_x)\n",
        "\n",
        "  if (scaler == 'stand'):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_x)\n",
        "    train_x = scaler.transform(train_x)\n",
        "    scaler.fit(valid_x)\n",
        "    valid_x = scaler.transform(valid_x)\n",
        "\n",
        "  if (scaler == 'log'):\n",
        "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "    train_x = transformer.transform(train_x)\n",
        "    valid_x = transformer.transform(valid_x)\n",
        "\n",
        "  # turning these train/tests into lgb/xgb datasets\n",
        "  dtrain_gbm = lgb.Dataset(train_x, label=train_y)\n",
        "  dvalid_gbm = lgb.Dataset(valid_x, label=valid_y)\n",
        "\n",
        "  dtrain_xbg = xgb.DMatrix(train_x, label=train_y)\n",
        "  dvalid_xbg = xgb.DMatrix(valid_x, label=valid_y)\n",
        "  return train_x, valid_x, train_y, valid_y,  dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvOJWxJkHWx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = ['lreg', 'las', 'rid','ada', 'et', 'rf', 'xgb', 'gbm'] # when you want to try all of them/ iteration: 1 \n",
        "# models = ['ada', 'et', 'rf', 'xgb', 'gbm'] # selecting the 5 best models from iteration 1 (by observing the log) / iteration: 2\n",
        "# models = ['las', 'rid', 'et', 'xgb'] \n",
        "comb = combinations(models, 3) \n",
        "comb = list(comb)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozV7fL7kt9Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Objective:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.best_gbm = None\n",
        "        self._gbm = None\n",
        "        self.best_xgb = None\n",
        "        self._xgb = None\n",
        "        self.predictions = None\n",
        "        self.fpredictions = None\n",
        "\n",
        "    def __call__(self, trial):\n",
        "\n",
        "        i = trial.suggest_int(\"combos\", 0, (len(comb)-1))\n",
        " \n",
        "        gbm_preds = 0\n",
        "        gbm_rmse = float('inf')\n",
        "        xgb_preds = 0\n",
        "        xgb_rmse = float('inf')\n",
        "        rf_preds = 0\n",
        "        rf_rmse = float('inf')\n",
        "        et_preds = 0\n",
        "        et_rmse = float('inf')\n",
        "        ada_preds = 0\n",
        "        ada_rmse = float('inf')\n",
        "        lreg_preds = 0\n",
        "        lreg_rmse = float('inf')\n",
        "        las_preds = 0\n",
        "        las_rmse = float('inf')\n",
        "        rid_preds = 0\n",
        "        rid_rmse = float('inf')\n",
        "\n",
        "        ###############################################################################\n",
        "        #                                 . Scaling                                   #\n",
        "        ###############################################################################\n",
        "        # If you want 1 scaled data for everything use this and comment out, other wise leave this commented\n",
        "        # scaler = trial.suggest_categorical(\"Scaler\", ['minmax','stand','log'])\n",
        "        # train_x, valid_x, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(scaler)\n",
        "        ###############################################################################\n",
        "        #                            . Linear Regression                              #\n",
        "        ###############################################################################\n",
        "        if any(x == 'lreg' for x in comb[i]):\n",
        "          lr_scaler = trial.suggest_categorical(\"lr_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(lr_scaler)\n",
        "          lreg = LinearRegression().fit(train_x, train_y)\n",
        "          lreg_preds = lreg.predict(valid_x)\n",
        "          lreg_rmse = sklearn.metrics.mean_squared_error(valid_y, lreg_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . Lasso                                     #\n",
        "        ###############################################################################\n",
        "        if any(x == 'las' for x in comb[i]):\n",
        "          las_scaler = trial.suggest_categorical(\"las_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(las_scaler)\n",
        "          alphal = trial.suggest_loguniform(\"alphal\", 1e-8, 1.0)\n",
        "          las = linear_model.Lasso(alpha=alphal).fit(train_x, train_y)\n",
        "          las_preds = las.predict(valid_x)\n",
        "          las_rmse = sklearn.metrics.mean_squared_error(valid_y, las_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . Ridge                                     #\n",
        "        ###############################################################################\n",
        "        if any(x == 'rid' for x in comb[i]):\n",
        "          rid_scaler = trial.suggest_categorical(\"rid_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(rid_scaler)\n",
        "          alphar = trial.suggest_loguniform(\"alphar\", 1e-8, 1.0)\n",
        "          rid = Ridge(alpha=alphar).fit(train_x, train_y)\n",
        "          rid_preds = rid.predict(valid_x)\n",
        "          rid_rmse = sklearn.metrics.mean_squared_error(valid_y, rid_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                               . Ada Boost                                   #\n",
        "        ###############################################################################\n",
        "        if any(x == 'ada' for x in comb[i]):\n",
        "          ada_scaler = trial.suggest_categorical(\"ada_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(ada_scaler)\n",
        "          ada_ne = trial.suggest_int(\"ada_ne\", 1, 500) #500\n",
        "          ada_lr = trial.suggest_loguniform(\"ada_lr\", 0.05, 1)\n",
        "          ada = AdaBoostRegressor(n_estimators = ada_ne, learning_rate = ada_lr,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          ada_preds = ada.predict(valid_x)\n",
        "          ada_rmse = sklearn.metrics.mean_squared_error(valid_y, ada_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                              . Extra Trees                                  #\n",
        "        ###############################################################################\n",
        "        if any(x == 'et' for x in comb[i]):\n",
        "          et_scaler = trial.suggest_categorical(\"et_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(et_scaler)\n",
        "          et_md = trial.suggest_int(\"et_max_depth\", 1, 100)\n",
        "          et_ne = trial.suggest_int(\"et_ne\", 1, 500) #1000\n",
        "          et = ExtraTreesRegressor(max_depth=et_md, n_estimators = et_ne,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          et_preds = et.predict(valid_x)\n",
        "          et_rmse = sklearn.metrics.mean_squared_error(valid_y, et_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                             . Random Forest                                 #\n",
        "        ###############################################################################\n",
        "        if any(x == 'rf' for x in comb[i]):\n",
        "          rf_scaler = trial.suggest_categorical(\"rf_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(rf_scaler)\n",
        "          rf_md = trial.suggest_int(\"rf_max_depth\", 1, 100)\n",
        "          rf_ne = trial.suggest_int(\"rf_ne\", 1, 500) #1000\n",
        "          rf = RandomForestRegressor(max_depth=rf_md, n_estimators = rf_ne,\n",
        "                                     random_state=0).fit(train_x, train_y)\n",
        "          rf_preds = rf.predict(valid_x)\n",
        "          rf_rmse = sklearn.metrics.mean_squared_error(valid_y, rf_preds, squared = False)\n",
        "        ###############################################################################\n",
        "        #                                 . XGBoost                                   #\n",
        "        ###############################################################################\n",
        "        if any(x == 'xgb' for x in comb[i]): \n",
        "          xgb_scaler = trial.suggest_categorical(\"xgb_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(xgb_scaler)\n",
        "          xgb_param = {\n",
        "              \"silent\": 1,\n",
        "              \"objective\": \"reg:squarederror\",\n",
        "              \"eval_metric\": \"rmse\",\n",
        "              \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "              \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
        "              \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n",
        "          }\n",
        "          if xgb_param[\"booster\"] == \"gbtree\" or xgb_param[\"booster\"] == \"dart\":\n",
        "              xgb_param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 100)\n",
        "              xgb_param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n",
        "              xgb_param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n",
        "              xgb_param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "          if xgb_param[\"booster\"] == \"dart\":\n",
        "              xgb_param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "              xgb_param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "              xgb_param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0)\n",
        "              xgb_param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n",
        "          # Add a callback for pruning.\n",
        "          xgb_pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\" )\n",
        "          xgb_ = xgb.train(xgb_param, dtrain_xbg, evals=[(dvalid_xbg, \"validation\")], verbose_eval=False, callbacks=[xgb_pruning_callback])\n",
        "          xgb_preds = xgb_.predict(dvalid_xbg)\n",
        "          xgb_rmse = sklearn.metrics.mean_squared_error(valid_y, xgb_preds, squared = False)\n",
        "          self._xgb = xgb_\n",
        "        ###############################################################################\n",
        "        #                          . Light Gradient Boosting                          #\n",
        "        ###############################################################################\n",
        "        if any(x == 'gbm' for x in comb[i]):\n",
        "          gbm_scaler = trial.suggest_categorical(\"gbm_Scaler\", ['minmax','stand','log'])\n",
        "          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(gbm_scaler)\n",
        "          gbm_param = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "              \"verbosity\": -1,\n",
        "              \"boosting_type\": \"gbdt\",\n",
        "              \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10), \n",
        "              \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10),\n",
        "              \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256), \n",
        "              \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0), \n",
        "              \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
        "              \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7), \n",
        "              \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2, 20), \n",
        "          }\n",
        "          # Add a callback for pruning.\n",
        "          gbm_pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n",
        "          gbm = lgb.train(gbm_param, dtrain_gbm, valid_sets=[dvalid_gbm], verbose_eval=False, callbacks=[gbm_pruning_callback])\n",
        "          gbm_preds = gbm.predict(valid_x)\n",
        "          gbm_rmse = sklearn.metrics.mean_squared_error(valid_y, gbm_preds, squared = False)\n",
        "          self._gbm = gbm\n",
        "        ###############################################################################\n",
        "        #                            . Stacking Strategy                              #\n",
        "        ###############################################################################\n",
        "\n",
        "        # strat = trial.suggest_int(\"strat\", 0, 1)\n",
        "\n",
        "        # if (strat == 0): # taking mean\n",
        "        preds = (gbm_preds + xgb_preds + rf_preds + \\\n",
        "        las_preds + ada_preds + et_preds + \\\n",
        "        lreg_preds + rid_preds) / 3\n",
        "\n",
        "        # if (strat == 1): # Making an equation\n",
        "        # a = trial.suggest_uniform(\"a\", 0.1, 0.9)\n",
        "        # b = trial.suggest_uniform(\"b\", 0.1, 0.9)\n",
        "        # c = trial.suggest_uniform(\"c\", 0.1, 0.9)\n",
        "        # d = trial.suggest_uniform(\"d\", 0.1, 0.9)\n",
        "\n",
        "        # def custom_sort(t):\n",
        "        #   return t[0]\n",
        "\n",
        "        # R = [[gbm_rmse,gbm_preds] , [xgb_rmse,xgb_preds], [rf_rmse, rf_preds], \\\n",
        "        #     [las_rmse,las_preds], [ada_rmse,ada_preds],[et_rmse,et_preds],\\\n",
        "        #     [lreg_rmse,lreg_preds], [rid_rmse,rid_preds]]\n",
        "        # R.sort(key=custom_sort) # low to high\n",
        "        # m1 = R[0][1]\n",
        "        # m2 = R[1][1]\n",
        "        # m3 = R[2][1]\n",
        "        # # Using list comprehension. This may result in a divide by 0 error when rounding occurs.\n",
        "        # # A fix would be to use a more precise library\n",
        "        # preds = [ (a*((x**b) + (y**c))) + (d*z) for x,y,z in zip(m1,m2,m3) ]\n",
        "        self.predictions = preds\n",
        "        rmse = sklearn.metrics.mean_squared_error(valid_y, preds, squared = False)\n",
        "        return rmse\n",
        "\n",
        "    def callback(self, study, trial):\n",
        "        if study.best_trial == trial:\n",
        "            self.best_gbm = self._gbm\n",
        "            self.best_xgb = self._xgb\n",
        "            self.fpredictions = self.predictions"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZdyvAQvuddv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "69546775-cfd6-4b38-f8bb-85ffa4c201cf"
      },
      "source": [
        "import optuna\n",
        "objective = Objective()\n",
        "\n",
        "# Setting SEED \n",
        "from optuna.samplers import TPESampler\n",
        "sampler = TPESampler(seed=10)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\",\n",
        "    sampler=sampler\n",
        ")\n",
        "study.optimize(objective, n_trials=10, callbacks=[objective.callback]) # change this to 500 + \n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "best_gbm = objective.best_gbm\n",
        "best_xgb = objective.best_xgb"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-07-11 04:41:17,537] Finished trial#0 with value: 126.57800321246911 with parameters: {'combos': 9, 'lr_Scaler': 'stand', 'rid_Scaler': 'minmax', 'alphar': 9.051476667906094e-05, 'xgb_Scaler': 'stand', 'booster': 'gbtree', 'lambda': 9.728728830009641e-05, 'alpha': 6.285982451182996e-07, 'max_depth': 9, 'eta': 1.3320760321578133e-06, 'gamma': 1.2847622527426482e-06, 'grow_policy': 'depthwise'}. Best is trial#0 with value: 126.57800321246911.\n",
            "[I 2020-07-11 04:44:45,842] Finished trial#1 with value: 9.52028670975306 with parameters: {'combos': 36, 'rid_Scaler': 'minmax', 'alphar': 0.034104735191002086, 'ada_Scaler': 'log', 'ada_ne': 345, 'ada_lr': 0.5704456208852126, 'et_Scaler': 'log', 'et_max_depth': 34, 'et_ne': 201}. Best is trial#1 with value: 9.52028670975306.\n",
            "[I 2020-07-11 04:46:59,255] Finished trial#2 with value: 24.311391864157027 with parameters: {'combos': 36, 'rid_Scaler': 'log', 'alphar': 0.006599090886310522, 'ada_Scaler': 'log', 'ada_ne': 78, 'ada_lr': 0.0741311954695703, 'et_Scaler': 'stand', 'et_max_depth': 14, 'et_ne': 349}. Best is trial#1 with value: 9.52028670975306.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations. Duality gap: 48126338.59771239, tolerance: 101084.04810864964\n",
            "\n",
            "[I 2020-07-11 04:49:33,862] Finished trial#3 with value: 23.7746290020224 with parameters: {'combos': 22, 'las_Scaler': 'log', 'alphal': 3.4250380089316126e-05, 'rid_Scaler': 'minmax', 'alphar': 6.969709355203061e-08, 'et_Scaler': 'stand', 'et_max_depth': 37, 'et_ne': 284}. Best is trial#1 with value: 9.52028670975306.\n",
            "[I 2020-07-11 05:02:30,059] Finished trial#4 with value: 131.39040449651083 with parameters: {'combos': 18, 'lr_Scaler': 'stand', 'rf_Scaler': 'stand', 'rf_max_depth': 23, 'rf_ne': 408, 'xgb_Scaler': 'log', 'booster': 'gbtree', 'lambda': 0.0007862247822146307, 'alpha': 4.519163409183482e-06, 'max_depth': 10, 'eta': 5.292669728952881e-08, 'gamma': 2.544488269752405e-06, 'grow_policy': 'lossguide'}. Best is trial#1 with value: 9.52028670975306.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations. Duality gap: 347572.31428290374, tolerance: 101084.04810864964\n",
            "\n",
            "[I 2020-07-11 05:02:40,458] Finished trial#5 with value: 126.57497934388431 with parameters: {'combos': 24, 'las_Scaler': 'stand', 'alphal': 2.3723051688905587e-08, 'rid_Scaler': 'minmax', 'alphar': 0.00024026430110458914, 'xgb_Scaler': 'log', 'booster': 'gbtree', 'lambda': 3.9046338723863027e-07, 'alpha': 0.0715817678062338, 'max_depth': 72, 'eta': 3.0076302325777034e-06, 'gamma': 0.0006034736645430906, 'grow_policy': 'depthwise'}. Best is trial#1 with value: 9.52028670975306.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations. Duality gap: 345984.8699345136, tolerance: 101084.04810864964\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/nanfunctions.py:1368: RuntimeWarning:\n",
            "\n",
            "Mean of empty slice\n",
            "\n",
            "[I 2020-07-11 05:06:26,115] Finished trial#6 with value: 11.32545512831097 with parameters: {'combos': 34, 'las_Scaler': 'minmax', 'alphal': 4.018769547074794e-06, 'rf_Scaler': 'minmax', 'rf_max_depth': 16, 'rf_ne': 135, 'gbm_Scaler': 'log', 'lambda_l1': 1.5192158253525843e-07, 'lambda_l2': 0.38491195620799024, 'num_leaves': 114, 'feature_fraction': 0.6304686692153197, 'bagging_fraction': 0.9665564273432807, 'bagging_freq': 7, 'min_child_samples': 19}. Best is trial#1 with value: 9.52028670975306.\n",
            "[I 2020-07-11 05:17:06,005] Finished trial#7 with value: 10.233595363956043 with parameters: {'combos': 15, 'lr_Scaler': 'minmax', 'et_Scaler': 'minmax', 'et_max_depth': 43, 'et_ne': 238, 'rf_Scaler': 'stand', 'rf_max_depth': 98, 'rf_ne': 307}. Best is trial#1 with value: 9.52028670975306.\n",
            "[I 2020-07-11 05:17:14,012] Setting status of trial#8 as TrialState.PRUNED. Trial was pruned at iteration 11.\n",
            "[I 2020-07-11 05:20:29,038] Finished trial#9 with value: 136.37684012386072 with parameters: {'combos': 41, 'rid_Scaler': 'log', 'alphar': 0.10587005575597368, 'et_Scaler': 'log', 'et_max_depth': 82, 'et_ne': 402, 'xgb_Scaler': 'minmax', 'booster': 'gbtree', 'lambda': 1.1926253263971827e-07, 'alpha': 0.0006883499422095858, 'max_depth': 65, 'eta': 2.5525277457416983e-06, 'gamma': 1.5966078359426927e-08, 'grow_policy': 'depthwise'}. Best is trial#1 with value: 9.52028670975306.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Params: \n",
            "    combos: 36\n",
            "    rid_Scaler: minmax\n",
            "    alphar: 0.034104735191002086\n",
            "    ada_Scaler: log\n",
            "    ada_ne: 345\n",
            "    ada_lr: 0.5704456208852126\n",
            "    et_Scaler: log\n",
            "    et_max_depth: 34\n",
            "    et_ne: 201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqw72dcpYYbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comb[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZf0muwDLZlB",
        "colab_type": "text"
      },
      "source": [
        "best result so far:\n",
        "\n",
        "**BINANCE ETH-USD**\n",
        "\n",
        "\n",
        "```\n",
        "6.55939309007308\n",
        "    ('las', 'rid', 'et')\n",
        "    las_Scaler: stand\n",
        "    alphal: 0.0004305476461811444\n",
        "    rid_Scaler: stand\n",
        "    alphar: 2.951911001774111e-07\n",
        "    et_Scaler: log\n",
        "    et_max_depth: 35\n",
        "    et_ne: 13\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**BOSTON HOUSE** \n",
        "\n",
        "3.067 : ('ada', 'et', 'xgb')\n",
        "\n",
        "```\n",
        "  Params: \n",
        "    combos: 1\n",
        "    ada_Scaler: log\n",
        "    ada_ne: 173\n",
        "    ada_lr: 0.6716180264894074\n",
        "    et_Scaler: stand\n",
        "    et_max_depth: 10\n",
        "    et_ne: 593\n",
        "    xgb_Scaler: log\n",
        "    booster: gbtree\n",
        "    lambda: 0.056298103046901415\n",
        "    alpha: 0.0005885345642231615\n",
        "    max_depth: 3\n",
        "    eta: 0.5865796691943621\n",
        "    gamma: 6.697097340043158e-08\n",
        "    grow_policy: depthwise\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hy4db904fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "objective.fpredictions "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}